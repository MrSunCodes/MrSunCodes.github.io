<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
<meta name="baidu-site-verification" content="8CbHshECuh" />
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="true">
  <meta name="baidu-site-verification" content="true">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-material.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Gemini","version":"7.7.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"PRXMR01NC1","apiKey":"2821ed1166fb5c3797b2f3c979cdb0b4","indexName":"hexo_search","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="第二个作业，是关于逻辑回归的，逻辑回归可以看做是线性回归的拓展。还是将步骤分为7步，实际上我第一遍做的时候类似第一次作业写的代码，获得的正确率大概是80%，说实话有点低，于是参考了一下老师的代码，并优化了一下，最后的正确率从大概是87.89%。这篇博客便是将作业思路做一下总结。">
<meta property="og:type" content="article">
<meta property="og:title" content="李宏毅机器学习-第二课作业">
<meta property="og:url" content="http://yoursite.com/2020/03/19/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%BA%8C%E8%AF%BE%E4%BD%9C%E4%B8%9A/index.html">
<meta property="og:site_name" content="MrSun Blog">
<meta property="og:description" content="第二个作业，是关于逻辑回归的，逻辑回归可以看做是线性回归的拓展。还是将步骤分为7步，实际上我第一遍做的时候类似第一次作业写的代码，获得的正确率大概是80%，说实话有点低，于是参考了一下老师的代码，并优化了一下，最后的正确率从大概是87.89%。这篇博客便是将作业思路做一下总结。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yoursite.com/2020/03/19/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%BA%8C%E8%AF%BE%E4%BD%9C%E4%B8%9A/1.png">
<meta property="og:image" content="http://yoursite.com/2020/03/19/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%BA%8C%E8%AF%BE%E4%BD%9C%E4%B8%9A/2.png">
<meta property="og:image" content="http://yoursite.com/2020/03/19/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%BA%8C%E8%AF%BE%E4%BD%9C%E4%B8%9A/3.png">
<meta property="og:image" content="http://yoursite.com/2020/03/19/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%BA%8C%E8%AF%BE%E4%BD%9C%E4%B8%9A/4.png">
<meta property="og:image" content="http://yoursite.com/2020/03/19/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%BA%8C%E8%AF%BE%E4%BD%9C%E4%B8%9A/5.png">
<meta property="article:published_time" content="2020-03-19T06:50:57.000Z">
<meta property="article:modified_time" content="2020-03-19T14:02:35.109Z">
<meta property="article:author" content="Chris Paul">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="Logistic Regression">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/03/19/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%BA%8C%E8%AF%BE%E4%BD%9C%E4%B8%9A/1.png">

<link rel="canonical" href="http://yoursite.com/2020/03/19/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%BA%8C%E8%AF%BE%E4%BD%9C%E4%B8%9A/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true
  };
</script>

  <title>李宏毅机器学习-第二课作业 | MrSun Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband">  	
    </div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">MrSun Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">欢迎私戳</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

  
</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
      <a href="https://github.com/MrSuncodes" class="github-corner" target="_blank" title="Follow me on GitHub" aria-label="Follow me on GitHub">
      <svg width="80" height="80" viewBox="0 0 250 250" style="fill:#222; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
	<path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path>
      </svg>
</a>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/19/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%BA%8C%E8%AF%BE%E4%BD%9C%E4%B8%9A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/pig.jpg">
      <meta itemprop="name" content="Chris Paul">
      <meta itemprop="description" content="山东大学(威海)17级在读计算机科学与技术学生，对计算机各个领域和方向保持热情，此为个人博客。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MrSun Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          李宏毅机器学习-第二课作业
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-19 14:50:57 / 修改时间：22:02:35" itemprop="dateCreated datePublished" datetime="2020-03-19T14:50:57+08:00">2020-03-19</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2020/03/19/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%BA%8C%E8%AF%BE%E4%BD%9C%E4%B8%9A/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/03/19/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%BA%8C%E8%AF%BE%E4%BD%9C%E4%B8%9A/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>第二个作业，是关于逻辑回归的，逻辑回归可以看做是线性回归的拓展。还是将步骤分为7步，实际上我第一遍做的时候类似第一次作业写的代码，获得的正确率大概是80%，说实话有点低，于是参考了一下老师的代码，并优化了一下，最后的正确率从大概是87.89%。这篇博客便是将作业思路做一下总结。</p>
<a id="more"></a>

<h1 id="Step1：明确目标"><a href="#Step1：明确目标" class="headerlink" title="Step1：明确目标"></a><strong>Step1：明确目标</strong></h1><img src="/2020/03/19/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%BA%8C%E8%AF%BE%E4%BD%9C%E4%B8%9A/1.png" class="" title="[李宏毅机器学习-第二课作业]">  
<img src="/2020/03/19/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%BA%8C%E8%AF%BE%E4%BD%9C%E4%B8%9A/2.png" class="" title="[李宏毅机器学习-第二课作业]">  

<p>给我们的一共是六个：输出结果的格式，需要你预测的不带标签的测试集，训练集，X_train,Y_train,X_test。实际上通过观察后三个文件，发现只需要后三个文件即可，后三个文件是老师事先帮我们将数据整理成csv格式并且全都是数字的数据。经过观察我们可以发现<br>训练数据共54256个，测试集大概20000多个，然后参数共510个。那么可以得出结论</p>
<ul>
<li><input disabled="" type="checkbox"> 模型的输入是510维</li>
<li><input disabled="" type="checkbox"> 模型输出是一个布尔值表示预测的是或不是。<br>所以最适合的模型是<strong>Logistic Regression model</strong>。</li>
</ul>
<h1 id="Step2：数据预处理"><a href="#Step2：数据预处理" class="headerlink" title="Step2：数据预处理"></a><strong>Step2：数据预处理</strong></h1><p>老师已经将数据帮我们处理好了，我们只需获取即可。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># 数据预处理，将读到的数据分别存入X_train，Y_train,X_test中</span><br><span class="line">np.random.seed(0)</span><br><span class="line">X_train_fpath &#x3D; &#39;.&#x2F;dataset&#x2F;X_train&#39;</span><br><span class="line">Y_train_fpath &#x3D; &#39;.&#x2F;dataset&#x2F;Y_train&#39;</span><br><span class="line">X_test_fpath &#x3D; &#39;.&#x2F;dataset&#x2F;X_test&#39;</span><br><span class="line">output_fpath &#x3D; &#39;.&#x2F;output_&#123;&#125;.csv&#39;</span><br><span class="line"></span><br><span class="line"># Parse csv files to numpy array</span><br><span class="line">with open(X_train_fpath) as f:</span><br><span class="line">    next(f)</span><br><span class="line">    X_train &#x3D; np.array([line.strip(&#39;\n&#39;).split(&#39;,&#39;)[1:] for line in f], dtype&#x3D;float)</span><br><span class="line">with open(Y_train_fpath) as f:</span><br><span class="line">    next(f)</span><br><span class="line">    Y_train &#x3D; np.array([line.strip(&#39;\n&#39;).split(&#39;,&#39;)[1] for line in f], dtype&#x3D;float)</span><br><span class="line">with open(X_test_fpath) as f:</span><br><span class="line">    next(f)</span><br><span class="line">    X_test &#x3D; np.array([line.strip(&#39;\n&#39;).split(&#39;,&#39;)[1:] for line in f], dtype&#x3D;float)</span><br></pre></td></tr></table></figure>

<p>以上做完之后，我们就按步骤继续进行。</p>
<h1 id="Step3：Normalize与训练集分类即其他后续训练所用到的函数定义"><a href="#Step3：Normalize与训练集分类即其他后续训练所用到的函数定义" class="headerlink" title="Step3：Normalize与训练集分类即其他后续训练所用到的函数定义"></a><strong>Step3：Normalize与训练集分类即其他后续训练所用到的函数定义</strong></h1><p>Normalize的方法还是使用标准差标准化方法。但需要注意的是，这一次我们将所有将会用到的操作全部写进函数，所以注意函数的参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># Normalize函数，对数据进行正规化</span><br><span class="line">def _normalize(X, train&#x3D;True, specified_column&#x3D;None, X_mean&#x3D;None, X_std&#x3D;None):  # 函数内定义的都是默认的参数，参数的意义底下有说明</span><br><span class="line">    # This function normalizes specific columns of X.</span><br><span class="line">    # The mean and standard variance of training data will be reused when processing testing data.</span><br><span class="line">    #</span><br><span class="line">    # Arguments:</span><br><span class="line">    #     X: data to be processed</span><br><span class="line">    #     train: &#39;True&#39; when processing training data, &#39;False&#39; for testing data</span><br><span class="line">    #     specific_column: indexes of the columns that will be normalized. If &#39;None&#39;, all columns</span><br><span class="line">    #         will be normalized.</span><br><span class="line">    #     X_mean: mean value of training data, used when train &#x3D; &#39;False&#39;</span><br><span class="line">    #     X_std: standard deviation of training data, used when train &#x3D; &#39;False&#39;</span><br><span class="line">    # Outputs:</span><br><span class="line">    #     X: normalized data</span><br><span class="line">    #     X_mean: computed mean value of training data</span><br><span class="line">    #     X_std: computed standard deviation of training data</span><br><span class="line"></span><br><span class="line">    if specified_column &#x3D;&#x3D; None:  # 如果等于None的话，意味着所有列都需要正规化</span><br><span class="line">        specified_column &#x3D; np.arange(X.shape[1])  # 新建一个数组，是0-X.shape[1]即0-509</span><br><span class="line">    if train:  # 如果train为True，那么表示处理training data，否则就处理testing data,即不再另算X_mean和X_std</span><br><span class="line">        X_mean &#x3D; np.mean(X[:, specified_column], 0).reshape(1, -1)  # 对X的所有行以及特定列的数组中求各列的平均值（因为axis的参数为0），然后重组为一行的数组</span><br><span class="line">        X_std &#x3D; np.std(X[:, specified_column], 0).reshape(1, -1)  # 同X_mean</span><br><span class="line"></span><br><span class="line">    X[:, specified_column] &#x3D; (X[:, specified_column] - X_mean) &#x2F; (X_std + 1e-8)  # X_std加入一个很小的数防止分母除以0</span><br><span class="line"></span><br><span class="line">    return X, X_mean, X_std</span><br></pre></td></tr></table></figure>
<p>接下来是将训练数据按比例拆成训练数据和验证数据。这样的好处上一次作业我有讲过，可以参考一下。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 将训练集拆成训练集和验证集，默认值是0.25，可以调</span><br><span class="line">def _train_dev_split(X, Y, dev_ratio&#x3D;0.25):</span><br><span class="line">    # This function spilts data into training set and development set.</span><br><span class="line">    train_size &#x3D; int(len(X) * (1 - dev_ratio))</span><br><span class="line">    return X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]</span><br></pre></td></tr></table></figure>

<p>函数定义完了，我们就使用函数进行处理即可。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># Normalize training and testing data</span><br><span class="line">X_train, X_mean, X_std &#x3D; _normalize(X_train, train&#x3D;True)</span><br><span class="line">X_test, _, _ &#x3D; _normalize(X_test, train&#x3D;False, X_mean&#x3D;X_mean, X_std&#x3D;X_std)</span><br><span class="line"></span><br><span class="line"># Split data into training set and development set，按9:1进行拆分</span><br><span class="line">dev_ratio &#x3D; 0.1</span><br><span class="line">X_train, Y_train, X_dev, Y_dev &#x3D; _train_dev_split(X_train, Y_train, dev_ratio&#x3D;dev_ratio)</span><br><span class="line"></span><br><span class="line"># 用来看看拆开的数据，以及数据参数维度是否正确</span><br><span class="line">train_size &#x3D; X_train.shape[0]</span><br><span class="line">dev_size &#x3D; X_dev.shape[0]</span><br><span class="line">test_size &#x3D; X_test.shape[0]</span><br><span class="line">data_dim &#x3D; X_train.shape[1]</span><br><span class="line">print(&#39;Size of training set: &#123;&#125;&#39;.format(train_size))</span><br><span class="line">print(&#39;Size of development set: &#123;&#125;&#39;.format(dev_size))</span><br><span class="line">print(&#39;Size of testing set: &#123;&#125;&#39;.format(test_size))</span><br><span class="line">print(&#39;Dimension of data: &#123;&#125;&#39;.format(data_dim))</span><br></pre></td></tr></table></figure>
<p>下面定义的函数，训练中会用到，具体的用法看我写的注释即可，已经非常明白了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"># 按顺序打乱X和Y，即打乱后，X[i]对应的仍是Y[i]</span><br><span class="line">def _shuffle(X, Y):</span><br><span class="line">    # This function shuffles two equal-length list&#x2F;array, X and Y, together.</span><br><span class="line">    randomize &#x3D; np.arange(len(X))  # 建立一个0-X的行-1的数组</span><br><span class="line">    np.random.shuffle(randomize)  # 生成大小为randomize的随机列表，</span><br><span class="line">    return (X[randomize], Y[randomize])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 定义sigmoid函数</span><br><span class="line">def _sigmoid(z):</span><br><span class="line">    # Sigmoid function can be used to calculate probability.</span><br><span class="line">    # To avoid overflow, minimum&#x2F;maximum output value is set.</span><br><span class="line">    # 为避免溢出，设置了最大最小值，即如果sigmoid函数的最小值比1e-8小，只会输出1e-8；而比1 - (1e-8)大，则只输出1 - (1e-8)</span><br><span class="line">    return np.clip(1 &#x2F; (1.0 + np.exp(-z)), 1e-8, 1 - (1e-8))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 逻辑回归的方程，输入是X，参数是w，bias是b，注意X与w都是数组，而b是一个数</span><br><span class="line"># 实际上这跟在X中新加一列全1作为b的相乘数的结果是一样的</span><br><span class="line">def _f(X, w, b):</span><br><span class="line">    # This is the logistic regression function, parameterized by w and b</span><br><span class="line">    #</span><br><span class="line">    # Arguements:</span><br><span class="line">    #     X: input data, shape &#x3D; [batch_size, data_dimension]</span><br><span class="line">    #     w: weight vector, shape &#x3D; [data_dimension, ]</span><br><span class="line">    #     b: bias, scalar</span><br><span class="line">    # Output:</span><br><span class="line">    #     predicted probability of each row of X being positively labeled, shape &#x3D; [batch_size, ]</span><br><span class="line">    # 在np.matmul(X, w)的基础上，数列中的每个值都加b得到最终的数列 matmul&#x3D;dot</span><br><span class="line">    return _sigmoid(np.matmul(X, w) + b)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 将sigmoid中获得的值四舍五入转换成0或1(int型)，注意如果正好为0.5，(虽然几率很小)结果是0</span><br><span class="line"># 实际上</span><br><span class="line">def _predict(X, w, b):</span><br><span class="line">    # This function returns a truth value prediction for each row of X</span><br><span class="line">    # by rounding the result of logistic regression function.</span><br><span class="line">    return np.round(_f(X, w, b)).astype(np.int)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 返回模型正确率</span><br><span class="line">def _accuracy(Y_pred, Y_label):</span><br><span class="line">    # This function calculates prediction accuracy</span><br><span class="line">    acc &#x3D; 1 - np.mean(np.abs(</span><br><span class="line">        Y_pred - Y_label))  # np.abs(Y_pred - Y_label) 如果预测正确，则结果是0，否则结果是1，那么我们求mean平均值的话所得值是1的概率(mean相当于 1的个数&#x2F;总个数),那么我们求0的概率就是1-it。这比我的方法两个for循环快多了</span><br><span class="line">    return acc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 交叉熵</span><br><span class="line">def _cross_entropy_loss(y_pred, Y_label):</span><br><span class="line">    # This function computes the cross entropy.</span><br><span class="line">    #</span><br><span class="line">    # Arguements:</span><br><span class="line">    #     y_pred: probabilistic predictions, float vector，即还未放入_predict函数中的_f函数的结果</span><br><span class="line">    #     Y_label: ground truth labels, bool vector 真正的结果，只有0和1两个元素</span><br><span class="line">    # Output:</span><br><span class="line">    #     cross entropy, scalar 输出是交叉熵是一个数,具体公式与推导可看笔记或Logistic Regression ppt11页</span><br><span class="line">    cross_entropy &#x3D; -np.dot(Y_label, np.log(y_pred)) - np.dot((1 - Y_label), np.log(1 - y_pred))</span><br><span class="line">    return cross_entropy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 返回要调整的w参数的gradient与b参数的gradient，具体公式与推导请看笔记或Logistic Regression ppt11页</span><br><span class="line">def _gradient(X, Y_label, w, b):</span><br><span class="line">    # This function computes the gradient of cross entropy loss with respect to weight w and bias b.</span><br><span class="line">    y_pred &#x3D; _f(X, w, b)  # 预测值，是float类型而不是bool</span><br><span class="line">    pred_error &#x3D; Y_label - y_pred  # 真实值-预测值，即误差</span><br><span class="line">    w_grad &#x3D; -np.sum(pred_error * X.T, 1)  # X.T就是X的转置，axis取值为1时代表将每一行的元素相加，实际上返回的是1行510列的数组</span><br><span class="line">    b_grad &#x3D; -np.sum(pred_error)  # 对b求偏微分后的结果，黑板上没有，但因为逻辑回归和线性回归的损失函数相似，可由线性回归对b进行求偏微分得到</span><br><span class="line">    return w_grad, b_grad</span><br></pre></td></tr></table></figure>

<h1 id="Step4：训练模型"><a href="#Step4：训练模型" class="headerlink" title="Step4：训练模型"></a><strong>Step4：训练模型</strong></h1><p>这次我们训练的方法使用小批次循环训练。使用小批次进行训练时，当整个训练集的所有小批次都用过后，将所有训练集打散并重新分成很多小批次，继续进行一次，知道特定的次数结束为止。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># Zero initialization for weights ans bias</span><br><span class="line"># 使用0初始化w和b参数</span><br><span class="line">w &#x3D; np.zeros((data_dim,))</span><br><span class="line">b &#x3D; np.zeros((1,))</span><br><span class="line"></span><br><span class="line"># Some parameters for training</span><br><span class="line">max_iter &#x3D; 20  # 迭代次数</span><br><span class="line">batch_size &#x3D; 8  # 训练的批次中的数据个数</span><br><span class="line">learning_rate &#x3D; 0.05  # 学习率</span><br><span class="line"></span><br><span class="line"># Keep the loss and accuracy at every iteration for plotting</span><br><span class="line"># 将每次迭代的损失和正确率都保存，以方便画出来</span><br><span class="line">train_loss &#x3D; []  # 训练集损失</span><br><span class="line">dev_loss &#x3D; []  # 验证集损失</span><br><span class="line">train_acc &#x3D; []  # 训练集正确率</span><br><span class="line">dev_acc &#x3D; []  # 验证集正确率</span><br><span class="line"></span><br><span class="line"># Calcuate the number of parameter updates</span><br><span class="line"># 记录参数更新的次数</span><br><span class="line">step &#x3D; 1</span><br><span class="line"></span><br><span class="line"># Iterative training</span><br><span class="line">for epoch in range(max_iter):  # max_iter迭代次数</span><br><span class="line">    # Random shuffle at the begging of each epoch</span><br><span class="line">    # 随机的将训练集X和Y按顺序打乱</span><br><span class="line">    X_train, Y_train &#x3D; _shuffle(X_train, Y_train)</span><br><span class="line"></span><br><span class="line">    # Mini-batch training</span><br><span class="line">    for idx in range(int(np.floor(train_size &#x2F; batch_size))):  # 每个批次8个数据，一共48830个数据，共48830&#x2F;8&#x3D;6103次批次</span><br><span class="line">        X &#x3D; X_train[idx * batch_size:(idx + 1) * batch_size]  # 分别取X和Y中的对应8个数据(每个批次8个数据)</span><br><span class="line">        Y &#x3D; Y_train[idx * batch_size:(idx + 1) * batch_size]</span><br><span class="line"></span><br><span class="line">        # Compute the gradient</span><br><span class="line">        # 计算w参数和b参数的梯度</span><br><span class="line">        w_grad, b_grad &#x3D; _gradient(X, Y, w, b)</span><br><span class="line"></span><br><span class="line">        # gradient descent update</span><br><span class="line">        # learning rate decay with time</span><br><span class="line">        # 更新参数，自适应学习率这次使用的是非常简单的学习率除以更新次数的根</span><br><span class="line">        w &#x3D; w - learning_rate &#x2F; np.sqrt(step) * w_grad</span><br><span class="line">        b &#x3D; b - learning_rate &#x2F; np.sqrt(step) * b_grad</span><br><span class="line"></span><br><span class="line">        step &#x3D; step + 1  # 更新次数+1</span><br><span class="line"></span><br><span class="line">    # Compute loss and accuracy of training set and development set</span><br><span class="line">    y_train_pred &#x3D; _f(X_train, w, b)  # 计算预测的值，注意此时数据格式为float</span><br><span class="line">    Y_train_pred &#x3D; np.round(y_train_pred)  # 将数据格式转换为bool类型</span><br><span class="line">    train_acc.append(_accuracy(Y_train_pred, Y_train))  # 将这一轮迭代的正确率记录下来</span><br><span class="line">    train_loss.append(_cross_entropy_loss(y_train_pred, Y_train) &#x2F; train_size)  # 将这一次迭代的损失记录下来</span><br><span class="line"></span><br><span class="line">    y_dev_pred &#x3D; _f(X_dev, w, b)  # 同样的方法处理验证集</span><br><span class="line">    Y_dev_pred &#x3D; np.round(y_dev_pred)</span><br><span class="line">    dev_acc.append(_accuracy(Y_dev_pred, Y_dev))</span><br><span class="line">    dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev) &#x2F; dev_size)</span><br><span class="line"></span><br><span class="line">print(&#39;Training loss: &#123;&#125;&#39;.format(train_loss[-1]))  # 输出最后依次迭代的结果</span><br><span class="line">print(&#39;Development loss: &#123;&#125;&#39;.format(dev_loss[-1]))</span><br><span class="line">print(&#39;Training accuracy: &#123;&#125;&#39;.format(train_acc[-1]))</span><br><span class="line">print(&#39;Development accuracy: &#123;&#125;&#39;.format(dev_acc[-1]))</span><br><span class="line"></span><br><span class="line">np.save(&#39;weight_hw2.npy&#39;, w)  # 将参数保存下来</span><br></pre></td></tr></table></figure>

<p>我们可以选择将训练集与验证集的loss和accuracy画出来，这样更直观</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"># Loss curve</span><br><span class="line">plt.plot(train_loss)</span><br><span class="line">plt.plot(dev_loss)</span><br><span class="line">plt.title(&#39;Loss&#39;)</span><br><span class="line">plt.legend([&#39;train&#39;, &#39;dev&#39;])</span><br><span class="line">plt.savefig(&#39;loss.png&#39;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"># Accuracy curve</span><br><span class="line">plt.plot(train_acc)</span><br><span class="line">plt.plot(dev_acc)</span><br><span class="line">plt.title(&#39;Accuracy&#39;)</span><br><span class="line">plt.legend([&#39;train&#39;, &#39;dev&#39;])</span><br><span class="line">plt.savefig(&#39;acc.png&#39;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<img src="/2020/03/19/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%BA%8C%E8%AF%BE%E4%BD%9C%E4%B8%9A/3.png" class="" title="[李宏毅机器学习-第二课作业]">  
<img src="/2020/03/19/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%BA%8C%E8%AF%BE%E4%BD%9C%E4%B8%9A/4.png" class="" title="[李宏毅机器学习-第二课作业]">  

<h1 id="Step5：预测testdata得到预测结果"><a href="#Step5：预测testdata得到预测结果" class="headerlink" title="Step5：预测testdata得到预测结果"></a><strong>Step5：预测testdata得到预测结果</strong></h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">w &#x3D; np.load(&#39;weight_hw2.npy&#39;)  # 加载参数</span><br><span class="line"></span><br><span class="line"># Predict testing labels</span><br><span class="line">predictions &#x3D; _predict(X_test, w, b)</span><br><span class="line">with open(output_fpath.format(&#39;logistic&#39;), &#39;w&#39;) as f:</span><br><span class="line">    f.write(&#39;id,label\n&#39;)</span><br><span class="line">    for i, label in enumerate(predictions):</span><br><span class="line">        f.write(&#39;&#123;&#125;,&#123;&#125;\n&#39;.format(i, label))</span><br><span class="line">        </span><br><span class="line"># Print out the most significant weights</span><br><span class="line"># 找到权重中最大的前十项，即关联结果的最紧密的参数</span><br><span class="line">ind &#x3D; np.argsort(np.abs(w))[::-1] #将数组从小到大排好后从最后往前取</span><br><span class="line">with open(X_test_fpath) as f:</span><br><span class="line">    content &#x3D; f.readline().strip(&#39;\n&#39;).split(&#39;,&#39;)</span><br><span class="line">features &#x3D; np.array(content)</span><br><span class="line">for i in ind[0:10]:</span><br><span class="line">    print(features[i], w[i])</span><br></pre></td></tr></table></figure>
<p>最后一步的意思是，找到影响最后结果最直接的参数是什么，这样看起来更直观一点。排序的规则就是看w参数的大小从大到小排序。</p>
<h1 id="Step6：generative-model的二元分类器"><a href="#Step6：generative-model的二元分类器" class="headerlink" title="Step6：generative model的二元分类器"></a><strong>Step6：generative model的二元分类器</strong></h1><p>老师上课共讲了两种方法，一种是逻辑回归，一种是generative model的二元分类器。只不过generative model的二元分类器的w和b是可以通过平均值和协方差直接求出来，而不需要梯度下降进行收敛获得。但最后结果generative model的二元分类器应该不会比逻辑回归更好，但这种方法我们也可以写下来，具体步骤和用到的Normalize函数和分类函数都一样，就不在赘述了，注释也写的很详细。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;</span><br><span class="line">数据预处理与正规化</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"># Parse csv files to numpy array</span><br><span class="line">with open(X_train_fpath) as f:</span><br><span class="line">    next(f)</span><br><span class="line">    X_train &#x3D; np.array([line.strip(&#39;\n&#39;).split(&#39;,&#39;)[1:] for line in f], dtype&#x3D;float)</span><br><span class="line">with open(Y_train_fpath) as f:</span><br><span class="line">    next(f)</span><br><span class="line">    Y_train &#x3D; np.array([line.strip(&#39;\n&#39;).split(&#39;,&#39;)[1] for line in f], dtype&#x3D;float)</span><br><span class="line">with open(X_test_fpath) as f:</span><br><span class="line">    next(f)</span><br><span class="line">    X_test &#x3D; np.array([line.strip(&#39;\n&#39;).split(&#39;,&#39;)[1:] for line in f], dtype&#x3D;float)</span><br><span class="line"></span><br><span class="line"># Normalize training and testing data</span><br><span class="line">X_train, X_mean, X_std &#x3D; _normalize(X_train, train&#x3D;True)</span><br><span class="line">X_test, _, _ &#x3D; _normalize(X_test, train&#x3D;False, specified_column&#x3D;None, X_mean&#x3D;X_mean, X_std&#x3D;X_std)</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">我们需要分别计算两个类别内的数据平均值𝝁1，𝝁2与协方差矩阵𝜮1，𝜮2。Classfication ppt15页</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"># Compute in-class mean</span><br><span class="line"># 分别将数据中的两个类别的数据分开，这样才可以计算两个类别的数据平均值𝝁1，𝝁2</span><br><span class="line">X_train_0 &#x3D; np.array([x for x, y in zip(X_train, Y_train) if y &#x3D;&#x3D; 0])  # 训练集中属于类别0的数据</span><br><span class="line">X_train_1 &#x3D; np.array([x for x, y in zip(X_train, Y_train) if y &#x3D;&#x3D; 1])  # 训练集中属于类别1的数据</span><br><span class="line"></span><br><span class="line">mean_0 &#x3D; np.mean(X_train_0, axis&#x3D;0)  # 𝝁1</span><br><span class="line">mean_1 &#x3D; np.mean(X_train_1, axis&#x3D;0)  # 𝝁2</span><br><span class="line"></span><br><span class="line"># Compute in-class covariance</span><br><span class="line"># 计算协方差矩阵𝜮1，𝜮2</span><br><span class="line"># 先用0初始化数组</span><br><span class="line">cov_0 &#x3D; np.zeros((data_dim, data_dim))  # 𝜮1</span><br><span class="line">cov_1 &#x3D; np.zeros((data_dim, data_dim))  # 𝜮2</span><br><span class="line"></span><br><span class="line"># 公式可看https:&#x2F;&#x2F;blog.csdn.net&#x2F;mr_hhh&#x2F;article&#x2F;details&#x2F;78490576</span><br><span class="line">for x in X_train_0:</span><br><span class="line">    cov_0 +&#x3D; np.dot(np.transpose([x - mean_0]), [x - mean_0]) &#x2F; X_train_0.shape[0]</span><br><span class="line">for x in X_train_1:</span><br><span class="line">    cov_1 +&#x3D; np.dot(np.transpose([x - mean_1]), [x - mean_1]) &#x2F; X_train_1.shape[0]</span><br><span class="line"></span><br><span class="line"># print(cov_0)</span><br><span class="line"># print(cov_1)</span><br><span class="line"></span><br><span class="line"># Shared covariance is taken as a weighted average of individual in-class covariance.</span><br><span class="line"># 为了减少参数的个数，采用同样的𝜮使其在两个类别上通用。  Classfication ppt22页，公式是23页</span><br><span class="line">cov &#x3D; (cov_0 * X_train_0.shape[0] + cov_1 * X_train_1.shape[0]) &#x2F; (X_train_0.shape[0] + X_train_1.shape[0])</span><br><span class="line"># print(cov)</span><br><span class="line"></span><br><span class="line"># 有了数据平均值和协方差矩阵，可以直接将唯一的权重矩阵与偏差向量计算出来 Classfication ppt33页</span><br><span class="line"># Compute inverse of covariance matrix.</span><br><span class="line"># Since covariance matrix may be nearly singular, np.linalg.inv() may give a large numerical error.</span><br><span class="line"># Via SVD decomposition, one can get matrix inverse efficiently and accurately.</span><br><span class="line"># 计算出协方差矩阵的逆，用inv表示</span><br><span class="line">u, s, v &#x3D; np.linalg.svd(cov, full_matrices&#x3D;False)</span><br><span class="line">inv &#x3D; np.matmul(v.T * 1 &#x2F; s, u.T)</span><br><span class="line">#</span><br><span class="line"># # Directly compute weights and bias  公式在33页</span><br><span class="line">w &#x3D; np.dot(inv, mean_0 - mean_1)</span><br><span class="line">b &#x3D; (-0.5) * np.dot(mean_0, np.dot(inv, mean_0)) + 0.5 * np.dot(mean_1, np.dot(inv, mean_1)) + np.log(float(X_train_0.shape[0]) &#x2F; X_train_1.shape[0])</span><br><span class="line"></span><br><span class="line">np.save(&#39;weight_hw2_generative.npy&#39;, w)  # 将参数保存下来</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">w &#x3D; np.load(&#39;weight_hw2_generative.npy&#39;)  # 加载参数</span><br><span class="line"># Compute accuracy on training set</span><br><span class="line">Y_train_pred &#x3D; 1 - _predict(X_train, w, b)</span><br><span class="line">print(&#39;Training accuracy: &#123;&#125;&#39;.format(_accuracy(Y_train_pred, Y_train))) #0.8735</span><br><span class="line"></span><br><span class="line"># Predict testing labels</span><br><span class="line">predictions &#x3D; 1 - _predict(X_test, w, b)</span><br><span class="line">with open(output_fpath.format(&#39;generative&#39;), &#39;w&#39;) as f:</span><br><span class="line">    f.write(&#39;id,label\n&#39;)</span><br><span class="line">    for i, label in enumerate(predictions):</span><br><span class="line">        f.write(&#39;&#123;&#125;,&#123;&#125;\n&#39;.format(i, label))</span><br><span class="line"></span><br><span class="line"># Print out the most significant weights</span><br><span class="line">ind &#x3D; np.argsort(np.abs(w))[::-1]</span><br><span class="line">with open(X_test_fpath) as f:</span><br><span class="line">    content &#x3D; f.readline().strip(&#39;\n&#39;).split(&#39;,&#39;)</span><br><span class="line">features &#x3D; np.array(content)</span><br><span class="line">for i in ind[0:10]:</span><br><span class="line">    print(features[i], w[i])</span><br></pre></td></tr></table></figure>

<h1 id="Step7：总结即优化"><a href="#Step7：总结即优化" class="headerlink" title="Step7：总结即优化"></a><strong>Step7：总结即优化</strong></h1><p>以上，我们就完成了作业2的所有内容。还是按道理可以继续优化，使用正则项，但这里我就没再写。<br>调参结果：对相同的批次(8)，相同的迭代次数(10)时，学习率0.05最好 0.8765 loss 0.2837<br>相同迭代次数10，学习率0.05，批次为10时，结果0.8759，loss0.2837<br>学习率0.05，批次为8时,迭代次数20 ，结果0.8789，loss0.2834<br>可以看出，最好的正确率大概是0.8789左右。本来想提交到kaggle上看看是否得分会提高，但不知为啥就算挂了梯子它也提交不上去，估计人太多了吧，下次再试试。<br>我自己第一遍写的代码以及最后获得的两个参数文件可以邮箱私聊找我获取。噶油！</p>
<img src="/2020/03/19/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%BA%8C%E8%AF%BE%E4%BD%9C%E4%B8%9A/5.png" class="" title="[李宏毅机器学习-第二课作业]">  
<p>在kaggle上的成绩又往前提高了hh，之前两个0.86是我自己写的得到的正确率，后来的0.88617是直接跑的老师的结果得到的，0.88972是调参之后的结果。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"><i class="fa fa-tag"></i> Machine Learning</a>
              <a href="/tags/Logistic-Regression/" rel="tag"><i class="fa fa-tag"></i> Logistic Regression</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/03/15/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E8%AF%BE%E4%BD%9C%E4%B8%9A/" rel="prev" title="李宏毅机器学习-第一课作业">
      <i class="fa fa-chevron-left"></i> 李宏毅机器学习-第一课作业
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/03/23/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-keras-demo/" rel="next" title="李宏毅机器学习-keras demo">
      李宏毅机器学习-keras demo <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Step1：明确目标"><span class="nav-number">1.</span> <span class="nav-text">Step1：明确目标</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Step2：数据预处理"><span class="nav-number">2.</span> <span class="nav-text">Step2：数据预处理</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Step3：Normalize与训练集分类即其他后续训练所用到的函数定义"><span class="nav-number">3.</span> <span class="nav-text">Step3：Normalize与训练集分类即其他后续训练所用到的函数定义</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Step4：训练模型"><span class="nav-number">4.</span> <span class="nav-text">Step4：训练模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Step5：预测testdata得到预测结果"><span class="nav-number">5.</span> <span class="nav-text">Step5：预测testdata得到预测结果</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Step6：generative-model的二元分类器"><span class="nav-number">6.</span> <span class="nav-text">Step6：generative model的二元分类器</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Step7：总结即优化"><span class="nav-number">7.</span> <span class="nav-text">Step7：总结即优化</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Chris Paul"
      src="/images/pig.jpg">
  <p class="site-author-name" itemprop="name">Chris Paul</p>
  <div class="site-description" itemprop="description">山东大学(威海)17级在读计算机科学与技术学生，对计算机各个领域和方向保持热情，此为个人博客。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">46</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/MrSuncodes" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;MrSuncodes" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/790567648@qq.com" title="E-Mail → 790567648@qq.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://leetcode-cn.com/problemset/all/" title="https:&#x2F;&#x2F;leetcode-cn.com&#x2F;problemset&#x2F;all&#x2F;" rel="noopener" target="_blank">leetcode</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://pintia.cn/problem-sets?tab=0/" title="https:&#x2F;&#x2F;pintia.cn&#x2F;problem-sets?tab&#x3D;0&#x2F;" rel="noopener" target="_blank">PAT</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://download.pytorch.org/whl/torch_stable.html" title="https:&#x2F;&#x2F;download.pytorch.org&#x2F;whl&#x2F;torch_stable.html" rel="noopener" target="_blank">Pytorch version</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chris Paul</span>
</div>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共43.3k字</span>
</div>

<div class="weixin-box">
  <div class="weixin-menu">
    <div class="weixin-hover"></div>
  </div>
</div>
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'we9Wk3cqQT9uzNMyUXovWLJc-gzGzoHsz',
      appKey     : '3QQaYG7aUFFq0QREe4BzzHOh',
      placeholder: "欢迎畅所欲言",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>